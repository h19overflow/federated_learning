# Scout Investigation Report
**Timestamp:** 2026-01-09 14:30:00
**Task:** Add router agent to decide between tool-augmented research vs basic conversational responses

---

## Root Cause / Requirements

The current ArxivAugmentedEngine always binds tools to the model and lets Gemini decide whether to use them (line 208 in engine.py). This causes the model to unnecessarily evaluate tool availability even for simple conversational queries. We need a lightweight router agent (similar to title_generator.py) that decides upfront whether a query needs tool-augmented research (RAG + arxiv + MCP) or can be answered with basic conversation. This will optimize response times and token usage for simple queries.

---

## Affected Domains

- **Backend Chat Engine** (Python/LangChain): [engine.py, arxiv_agent_prompts.py]
- **New Router Module** (Python/LangChain): [query_router.py - new file]

---

## Recommended Changes

### Backend Chat Engine

#### engine.py:66-71
- Current model initialization uses "gemini-3-pro-preview" for all queries
- Need to keep this as the main research agent
- Add router_model initialization separate from main llm

#### engine.py:172-312 (query_stream method)
- Line 172-178: Add router decision before tool retrieval
- Line 198-204: Current code retrieves tools unconditionally - should be conditional based on router
- Line 208: Tool binding should only happen if router selects "research" mode
- Line 292-302: Direct streaming path already exists for non-tool responses - should be used for "basic" mode

#### engine.py:111-140 (_get_tools method)
- Currently called unconditionally at line 198
- Should only be called if router decides "research" mode needed
- No changes to method itself, just when it's called

### New Router Module (query_router.py)

**Create:** `C:\Users\User\Projects\FYP2\federated_pneumonia_detection\src\control\agentic_systems\multi_agent_systems\chat\query_router.py`

**Purpose:** Lightweight classification agent using Gemini 2.0 Flash
**Pattern:** Follow title_generator.py structure (singleton model, simple function)
**Inputs:** User query string
**Output:** "research" or "basic" mode

**Logic:**
- Use gemini-2.0-flash-exp (fast, cheap)
- Temperature: 0.0 (deterministic classification)
- Max tokens: 10 (single word response)
- Classification criteria:
  - "research": Questions about papers, specific research topics, comparisons, technical details requiring sources
  - "basic": Greetings, clarifications, simple explanations, follow-ups on already-retrieved info

### Prompts Module

#### arxiv_agent_prompts.py:8-20 (ARXIV_AGENT_SYSTEM_PROMPT)
- Current prompt assumes tools are always available
- Need two variants:
  1. RESEARCH_MODE_PROMPT: Current prompt with tool instructions (lines 10-17)
  2. BASIC_MODE_PROMPT: Conversational prompt without tool references, emphasizes conciseness

**New exports needed:**
- ROUTER_CLASSIFICATION_PROMPT (for router agent)
- RESEARCH_MODE_SYSTEM_PROMPT (current prompt, renamed)
- BASIC_MODE_SYSTEM_PROMPT (new concise conversational prompt)

#### arxiv_agent_prompts.py:8-20
- Split existing ARXIV_AGENT_SYSTEM_PROMPT into:
  - Keep tool descriptions for research mode
  - Create basic mode variant: "You are a Scientific Research Assistant. Provide concise, helpful answers based on conversation context. Be brief and direct."

---

## Shared Interfaces

**Between Router and Engine:**
- Input: Query string (str)
- Output: Classification string ("research" | "basic")
- Function signature: `classify_query(query: str) -> str`

**Between Engine and Prompts:**
- Engine must select correct system prompt based on router decision
- Lines 159 in engine.py: `SystemMessage(content=ARXIV_AGENT_SYSTEM_PROMPT)`
- Change to: `SystemMessage(content=RESEARCH_MODE_PROMPT if mode == "research" else BASIC_MODE_PROMPT)`

**No changes to external API:**
- query_stream() signature remains unchanged
- SSE event types remain unchanged
- Frontend integration unaffected

---

## Implementation Complexity

**Medium**

**Reasoning:**
- New file creation (query_router.py): ~90 lines following title_generator.py pattern
- Engine modifications: ~20 lines across 3 locations (router call, conditional tool loading, prompt selection)
- Prompt refactoring: ~30 lines (split existing prompt, add router prompt)
- Total: ~140 lines, but follows existing patterns closely

**Risk factors:**
- Router classification quality depends on prompt engineering
- Need to ensure basic mode doesn't accidentally block legitimate research queries
- Tool binding logic needs careful refactoring to avoid breaking existing tool execution

---

## Additional Notes

### Router Classification Examples

**Should classify as "research":**
- "What papers discuss federated learning for medical imaging?"
- "Compare ResNet and VGG for pneumonia detection"
- "Show me recent arxiv papers on differential privacy"
- "Find papers by Andrew Ng on medical AI"

**Should classify as "basic":**
- "Hello"
- "Thanks!"
- "Can you explain that last point more?"
- "What did you mean by recall optimization?"
- "Summarize what you just said"

### Testing Considerations

1. **Router accuracy:** Test with 20+ example queries (10 research, 10 basic) to ensure >95% accuracy
2. **Performance:** Measure latency difference:
   - Current: Always loads tools + model decides (~200-300ms overhead)
   - With router: Router decision (~50ms) + conditional tool loading (0ms for basic, 200ms for research)
   - Expected improvement: ~150-250ms for basic queries
3. **Edge cases:**
   - Mixed queries: "Hi! Can you find papers on federated learning?" (should be "research")
   - Follow-up questions: "Tell me more about that paper" (context-dependent - may need conversation history)
4. **Graceful degradation:** If router fails, default to "research" mode (safer)

### Implementation Order

1. Create query_router.py (isolated, testable)
2. Add router prompts to arxiv_agent_prompts.py
3. Integrate router into engine.py (call before tool loading)
4. Refactor prompt selection based on router decision
5. Test router classification accuracy
6. Test end-to-end with both modes

### Files NOT Modified

- streaming.py: No changes needed (SSE events unchanged)
- mcp_manager.py: No changes (tool loading logic stays same, just called conditionally)
- rag_tool.py: No changes (tool implementation unchanged)
- history.py: No changes (history management unchanged)
- content.py: No changes (content processing unchanged)

### Performance Impact

**Before:**
- Every query: Load tools (100-200ms) + Bind tools + Model evaluates tool availability
- Token usage: Higher (tool descriptions in context)

**After:**
- Basic queries: Router only (~50ms) + Direct LLM streaming
- Research queries: Router (~50ms) + Load tools (100-200ms) + Tool-augmented generation
- Token savings: ~500-1000 tokens per basic query (no tool descriptions)

**Expected distribution:**
- 40% basic queries (greetings, clarifications, follow-ups)
- 60% research queries (actual research questions)
- Average latency improvement: ~60-100ms across all queries
- Token cost reduction: ~20-30% overall
