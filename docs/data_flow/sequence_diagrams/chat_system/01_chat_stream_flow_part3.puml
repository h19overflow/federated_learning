@startuml
!define RECTANGLE class

skinparam sequence {
    ArrowColor #333333
    ArrowThickness 2
    ActorBackgroundColor #F0F0F0
    ActorBorderColor #333333
    ParticipantBackgroundColor #FFFFFF
    ParticipantBorderColor #333333
    ParticipantFontSize 12
    ParticipantFontStyle bold
    LifeLineBackgroundColor #F0F0F0
    LifeLineBorderColor #999999
    BoxPadding 10
    MessagePadding 15
}

skinparam note {
    BackgroundColor #FFFFCC
    BorderColor #999999
    FontSize 11
}

title Chat Streaming Flow - Step 3: Stream Execution - Token-by-Token Response

' Participants
participant API as "**API**\nchat_stream.py" #FFF3E0
participant Agent as "**Agent**\nArxivAugmentedEngine" #E8EAF6
participant StreamFunc as "**StreamFunc**\nstream_query()" #E0F7FA
participant LLM as "**LLM**\nChatGoogleGenerativeAI" #FFEBEE
participant RAGTool as "**RAGTool**\nRAG Tool" #F3E5F5
participant ArxivTool as "**ArxivTool**\nArxiv MCP Tools" #E8F5E9
participant History as "**History**\nChatHistoryManager" #FFF3E0
participant Frontend as "**Frontend**\nSSE Client" #E3F2FD

autonumber

== Step 3: Stream Execution - Token-by-Token Response ==

' File: chat_stream.py lines 75-77
API -> Agent: async for event in agent.stream(chat_input)
activate Agent

note right of API
    **File:** chat_stream.py lines 75-77
end note

' File: research_engine.py lines 118-127
Agent -> StreamFunc: async for event in stream_query(llm, history_mgr, rag_tool, query, session_id, arxiv_enabled)
activate StreamFunc

note right of Agent
    **File:** research_engine.py lines 118-127
end note

StreamFunc -> StreamFunc: Build system prompt + conversation history
note right of StreamFunc
    **Action:** Get previous turns from history_mgr
end note

StreamFunc -> LLM: astream_events() with tools
activate LLM

note right of StreamFunc
    **Action:** Bind RAG tool + Arxiv tools if enabled
end note

loop For each LLM event
    alt Event: on_chat_model_stream
        LLM -> StreamFunc: Token chunk
        StreamFunc -> API: yield {type: "token", content: chunk}
        API -> Frontend: data: {"type":"token","content":"..."}
        
        note right of Frontend
            **SSE Event Received:**
            Token appended to message display
        end note
        
    else Event: on_tool_start (RAG)
        LLM -> RAGTool: invoke(query)
        activate RAGTool
        
        note right of LLM
            **Tool Call Detected:**
            RAG search initiated
        end note
        
        StreamFunc -> API: yield {type: "tool_call", tool: "rag_search", args: {...}}
        API -> Frontend: data: {"type":"tool_call",...}
        
        RAGTool -> RAGTool: Query vector store
        RAGTool --> LLM: Tool result
        deactivate RAGTool
        
    else Event: on_tool_start (arxiv)
        LLM -> ArxivTool: search_arxiv(query)
        activate ArxivTool
        
        StreamFunc -> API: yield {type: "tool_call", tool: "arxiv_search", args: {...}}
        API -> Frontend: data: {"type":"tool_call",...}
        
        ArxivTool -> ArxivTool: Search arxiv papers
        ArxivTool --> LLM: Papers found
        deactivate ArxivTool
        
    else Event: on_chat_model_end
        LLM -> StreamFunc: Final AI message
        StreamFunc -> StreamFunc: Extract full response
    end
end

deactivate LLM

' File: research_stream.py (delegated)
StreamFunc -> History: add_to_history(session_id, user_msg, ai_response)
activate History

note right of History
    **File:** chat/history/postgres_history.py
    **Action:** Persist conversation turn
end note

History --> StreamFunc: Saved
deactivate History

StreamFunc -> API: yield {type: "done"}
deactivate StreamFunc

API -> Frontend: data: {"type":"done"}
deactivate Agent

' File: chat_stream.py lines 79-81
API -> API: chunk_count check
activate API

note right of API
    **File:** chat_stream.py lines 79-81
end note

alt chunk_count == 0
    API -> Frontend: data: {"type":"error","message":"No response generated"}
end

deactivate API

' Continue to Step 4
note over API: Continue to SSE Response Formatting...

@enduml
